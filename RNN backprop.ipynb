{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3515bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd61fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log loss because binary outputs\n",
    "# sigmoid as activation funciton for hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d23178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y_hat, y):\n",
    "    \"\"\"\n",
    "    y_hat is the prediction\n",
    "    y is the true value\n",
    "    \"\"\"\n",
    "    if y == 1:\n",
    "        return -math.log(y_hat)\n",
    "    elif y == 0:\n",
    "        return -math.log(1-y_hat)\n",
    "    else:\n",
    "        raise InvalidInputError(\"y must be 0 or 1 but it is \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ed18f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + (math.e**-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655a5a1",
   "metadata": {},
   "source": [
    "## Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21e9896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def by_derivative(y_hat, y):\n",
    "    \"\"\"\n",
    "    y bias derivative with respect to log loss\n",
    "    \"\"\"\n",
    "    return (y_hat-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee7261ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wy_derivative(y_hat, y, a):\n",
    "    \"\"\"\n",
    "    y weights derivative with respect to log loss\n",
    "    \"\"\"\n",
    "    return by_derivative(y_hat,y)*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b6c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ba_derivative(y_hat, y, Wy, an):\n",
    "    \"\"\"\n",
    "    x bias derivative\n",
    "    \"\"\"\n",
    "    return (y_hat-y)*Wy*an*(1-an)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d35be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Waa_derivative(y_hat, y, Wy, an, a_prev):\n",
    "    return ba_derivative(y_hat, y, Wy, an) * a_prev\n",
    "\n",
    "def Wax_derivative(y_hat, y, Wy, an, x):\n",
    "    return ba_derivative(y_hat, y, Wy, an) * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3ff734",
   "metadata": {},
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32f6c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for i in range(1000):\n",
    "    sequence_length = random.randint(4,10)\n",
    "    \n",
    "    current_sequence = []\n",
    "    for j in range(sequence_length):\n",
    "        current_sequence.append(random.choice([0,1]))\n",
    "        \n",
    "    X.append(current_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1442f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for sequence in X:\n",
    "    shifted_sequence = [0] + sequence[:-1]\n",
    "    y.append(shifted_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b22560e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 1, 1, 0, 1, 0, 0, 0],\n",
       " [0, 1, 0, 1, 1, 1],\n",
       " [1, 1, 0, 1, 1, 0],\n",
       " [0, 0, 1, 1, 1, 0, 1],\n",
       " [1, 0, 0, 0, 0, 1, 0],\n",
       " [1, 1, 1, 1, 1, 0, 0, 0, 1],\n",
       " [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       " [0, 1, 1, 0, 0]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c23e5add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 1, 0, 1, 0, 0],\n",
       " [0, 0, 1, 0, 1, 1],\n",
       " [0, 1, 1, 0, 1, 1],\n",
       " [0, 0, 0, 1, 1, 1, 0],\n",
       " [0, 1, 0, 0, 0, 0, 1],\n",
       " [0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 1, 1, 1, 0, 0],\n",
       " [0, 0, 1, 1, 0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e245c",
   "metadata": {},
   "source": [
    "## Define the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "693cca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self):\n",
    "        self.Wy_first = random.gauss(0, 0.2)\n",
    "        self.Wy_second = random.gauss(0, 0.2)\n",
    "        self.by = random.gauss(0, 0.2)\n",
    "        \n",
    "        self.Waa_first_1 = random.gauss(0, 0.2)\n",
    "        self.Waa_first_2 = random.gauss(0, 0.2)\n",
    "        self.ba_first = random.gauss(0, 0.2)\n",
    "        \n",
    "        self.Waa_second_1 = random.gauss(0, 0.2)\n",
    "        self.Waa_second_2 = random.gauss(0, 0.2)\n",
    "        self.ba_second = random.gauss(0, 0.2)\n",
    "        \n",
    "        self.Wax_first = random.gauss(0, 0.2)\n",
    "        self.Wax_second = random.gauss(0, 0.2)\n",
    "\n",
    "        self.a_first = 0\n",
    "        self.a_second = 0\n",
    "        \n",
    "        # an example of correct output parameters\n",
    "        self.Wy_first = 0 \n",
    "        self.Wy_second = 10 # correct\n",
    "        self.by = -5 # correct\n",
    "        \n",
    "        self.Waa_first_1 = 0\n",
    "        self.Waa_first_2 = 0\n",
    "        self.ba_first = 5 # -5\n",
    "        \n",
    "        self.Waa_second_1 = 10\n",
    "        self.Waa_second_2 = 0\n",
    "        self.ba_second = -5\n",
    "        \n",
    "        self.Wax_first = 10\n",
    "        self.Wax_second = 0\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def predict(self, x_input):\n",
    "        \"\"\"\n",
    "        x_input is a list\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        \n",
    "        # set the initial value for a\n",
    "        current_a_first = self.a_first\n",
    "        current_a_second = self.a_second\n",
    "        \n",
    "        for x in x_input:\n",
    "            current_y_hat, current_a_first, current_a_second = self.run(x, current_a_first, current_a_second)\n",
    "            \n",
    "            output.append(current_y_hat)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def run(self, x, a_first, a_second):\n",
    "        \"\"\"\n",
    "        run the input through the rnn to get the an and y_hat \n",
    "        \"\"\"\n",
    "        current_a_first = sigmoid(self.Waa_first_1*a_first + self.Waa_first_2*a_second + self.Wax_first*x+ self.ba_first)\n",
    "        current_a_second = sigmoid(self.Waa_second_1*a_first + self.Waa_second_2*a_second + self.Wax_second*x+ self.ba_second)\n",
    "        \n",
    "        current_y_hat = sigmoid(self.Wy_first*current_a_first + self.Wy_second*current_a_second +self.by)\n",
    "        \n",
    "        return current_y_hat, current_a_first, current_a_second\n",
    "    \n",
    "    \n",
    "    def forward_pass(self, x, y, a_first, a_second):\n",
    "        \"\"\"\n",
    "        do a single forward pass of the training phase and calculate gradients\n",
    "        \n",
    "        this forward pass represents a single time step of the RNN\n",
    "        \"\"\"\n",
    "        \n",
    "        Wy_first_gradient = None # done\n",
    "        Wy_second_gradient = None # done\n",
    "        by_gradient = None # done\n",
    "        \n",
    "        Waa_first_1_gradient = None # done\n",
    "        Waa_first_2_gradient = None # done\n",
    "        ba_first_gradient = None # done\n",
    "        \n",
    "        Waa_second_1_gradient = None\n",
    "        Waa_second_2_gradient = None\n",
    "        ba_second_gradient = None # done\n",
    "        \n",
    "        Wax_first_gradient = None # done\n",
    "        Wax_second_gradient = None\n",
    "        \n",
    "        # predict the y_hat and activations a\n",
    "        y_hat, current_a_first, current_a_second = self.run(x, a_first, a_second)\n",
    "        \n",
    "        by_gradient = by_derivative(y_hat, y)\n",
    "        Wy_first_gradient = Wy_derivative(y_hat, y, current_a_first)\n",
    "        Wy_second_gradient = Wy_derivative(y_hat, y, current_a_second)\n",
    "        \n",
    "        ba_first_gradient = ba_derivative(y_hat, y, self.Wy_first, current_a_first)\n",
    "        Wax_first_gradient = Waa_derivative(y_hat, y, self.Wy_first, current_a_first, x)\n",
    "        Waa_first_1_gradient = Waa_derivative(y_hat, y, self.Wy_first, current_a_first, a_first)\n",
    "        Waa_first_2_gradient = Waa_derivative(y_hat, y, self.Wy_first, current_a_first, a_second)\n",
    "        \n",
    "        ba_second_gradient = ba_derivative(y_hat, y, self.Wy_second, current_a_second)\n",
    "        Wax_second_gradient = Waa_derivative(y_hat, y, self.Wy_second, current_a_second, x)\n",
    "        Waa_second_1_gradient = Waa_derivative(y_hat, y, self.Wy_second, current_a_second, a_first)\n",
    "        Waa_second_2_gradient = Waa_derivative(y_hat, y, self.Wy_second, current_a_second, a_second)\n",
    "        \n",
    "        gradients = (Wy_first_gradient, Wy_second_gradient, by_gradient, \n",
    "                     Waa_first_1_gradient, Waa_first_2_gradient, ba_first_gradient, \n",
    "                     Waa_second_1_gradient, Waa_second_2_gradient, ba_second_gradient, \n",
    "                     Wax_first_gradient, Wax_second_gradient)\n",
    "        \n",
    "        return gradients, current_a_first, current_a_second, y_hat\n",
    "\n",
    "    \n",
    "    \n",
    "    def update_weights(self, x_input, y_input, learning_rate=0.0001, print_loss=False):\n",
    "        \"\"\"\n",
    "        calculate the gradients and update the weights for a single datapoint\n",
    "        \"\"\"\n",
    "        a1 = self.a_first\n",
    "        a2 = self.a_second\n",
    "        \n",
    "        Wy_first_gradient_sum = 0\n",
    "        Wy_second_gradient_sum = 0\n",
    "        by_gradient_sum = 0\n",
    "        \n",
    "        Waa_first_1_gradient_sum = 0\n",
    "        Waa_first_2_gradient_sum = 0\n",
    "        ba_first_gradient_sum = 0\n",
    "        \n",
    "        Waa_second_1_gradient_sum = 0\n",
    "        Waa_second_2_gradient_sum = 0\n",
    "        ba_second_gradient_sum = 0\n",
    "        \n",
    "        Wax_first_gradient_sum = 0\n",
    "        Wax_second_gradient_sum = 0\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(len(x_input)):\n",
    "            x = x_input[i]\n",
    "            y = y_input[i]\n",
    "            gradients, a1, a2, y_hat = self.forward_pass(x, y, a1, a2)\n",
    "            \n",
    "#             print(\"gradients\", gradients)\n",
    "            (Wy_first_gradient, Wy_second_gradient, by_gradient, \n",
    "                     Waa_first_1_gradient, Waa_first_2_gradient, ba_first_gradient, \n",
    "                     Waa_second_1_gradient, Waa_second_2_gradient, ba_second_gradient, \n",
    "                     Wax_first_gradient, Wax_second_gradient) = gradients\n",
    "            \n",
    "#             print(\"by_gradient\", by_gradient)\n",
    "            # sum up the gradients for the entire sequence\n",
    "            Wy_first_gradient_sum += Wy_first_gradient\n",
    "            Wy_second_gradient_sum += Wy_second_gradient\n",
    "            by_gradient_sum += by_gradient\n",
    "            Waa_first_1_gradient_sum += Waa_first_1_gradient\n",
    "            Waa_first_2_gradient_sum += Waa_first_2_gradient\n",
    "            ba_first_gradient_sum += ba_first_gradient\n",
    "            Waa_second_1_gradient_sum += Waa_second_1_gradient\n",
    "            Waa_second_2_gradient_sum += Waa_second_2_gradient\n",
    "            ba_second_gradient_sum += ba_second_gradient\n",
    "            Wax_first_gradient_sum += Wax_first_gradient\n",
    "            Wax_second_gradient_sum += Wax_second_gradient\n",
    "            \n",
    "            loss += log_loss(y_hat, y)\n",
    "        \n",
    "        # print the loss for the individual sequence\n",
    "        if print_loss:\n",
    "            print(\"logistic loss\", loss)\n",
    "            \n",
    "        # gradient descent update\n",
    "        self.Wy_first = self.Wy_first - learning_rate * Wy_first_gradient_sum\n",
    "        self.Wy_second = self.Wy_second - learning_rate * Wy_second_gradient_sum\n",
    "        self.by = self.by - learning_rate * by_gradient_sum\n",
    "        self.Waa_first_1 = self.Waa_first_1 - learning_rate * Waa_first_1_gradient_sum\n",
    "        self.Waa_first_2 = self.Waa_first_2 - learning_rate * Waa_first_2_gradient_sum\n",
    "        self.ba_first = self.ba_first - learning_rate * ba_first_gradient_sum\n",
    "        self.Waa_second_1 = self.Waa_second_1 - learning_rate * Waa_second_1_gradient_sum\n",
    "        self.Waa_second_2 = self.Waa_second_2 - learning_rate * Waa_second_2_gradient_sum\n",
    "        self.ba_second = self.ba_second - learning_rate * ba_second_gradient_sum\n",
    "        self.Wax_first = self.Wax_first - learning_rate * Wax_first_gradient_sum\n",
    "        self.Wax_second = self.Wax_second - learning_rate * Wax_second_gradient_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c71c10",
   "metadata": {},
   "source": [
    "## Train the RNN to do a right shift of values. \n",
    "For example, [1, 0, 1] shifted right would be [0, 1, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5faf02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c80dce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.007152809912960745, 0.9928144512645394]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c397f774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ba_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59098ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic loss 0.014357029257883496\n"
     ]
    }
   ],
   "source": [
    "model.update_weights([1, 1],[0, 1], learning_rate=1, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1ede374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ba_first # should be 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d44046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f72f448f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.007153145366154462,\n",
       " 0.992864501270733,\n",
       " 0.9928970229907063,\n",
       " 0.9928970229906062,\n",
       " 0.992864502850085]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([0,1,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8478fcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic loss 9.928166952466437\n",
      "logistic loss 4.088919006446303\n",
      "logistic loss 2.9160551440223093\n",
      "logistic loss 2.8897309059567107\n",
      "logistic loss 2.86224539472021\n",
      "logistic loss 2.842607826733218\n",
      "logistic loss 2.8287772946719314\n",
      "logistic loss 2.8189966987332955\n",
      "logistic loss 2.812089573005358\n",
      "logistic loss 2.807291844535451\n",
      "logistic loss 2.8041059153690697\n",
      "logistic loss 2.8022051601248297\n",
      "logistic loss 2.8013751552837376\n",
      "logistic loss 2.801477950493884\n",
      "logistic loss 2.80243039837769\n",
      "logistic loss 2.804191202369968\n",
      "logistic loss 2.806753621456246\n",
      "logistic loss 2.81014214578204\n",
      "logistic loss 2.814412320981436\n",
      "logistic loss 2.8196535105787746\n",
      "logistic loss 2.8259949203486734\n",
      "logistic loss 2.8336158117614794\n",
      "logistic loss 2.842761666789876\n",
      "logistic loss 2.8537693618750968\n",
      "logistic loss 2.867106502910218\n",
      "logistic loss 2.8834333843384576\n",
      "logistic loss 2.9037005871933927\n",
      "logistic loss 2.929298039646864\n",
      "logistic loss 2.9622545346997002\n",
      "logistic loss 3.005369011297642\n",
      "logistic loss 3.0616388699149732\n",
      "logistic loss 3.1308680110569336\n",
      "logistic loss 3.201319855710679\n",
      "logistic loss 3.249489764746062\n",
      "logistic loss 3.2655377385809703\n",
      "logistic loss 3.2598980629796013\n",
      "logistic loss 3.243782024404643\n",
      "logistic loss 3.222891712977511\n",
      "logistic loss 3.199594009361133\n",
      "logistic loss 3.1748345175701616\n",
      "logistic loss 3.148996705236886\n",
      "logistic loss 3.1222409875704193\n",
      "logistic loss 3.0946369577474924\n",
      "logistic loss 3.0662162603158944\n",
      "logistic loss 3.036994599875231\n",
      "logistic loss 3.006981360260774\n",
      "logistic loss 2.976184047614574\n",
      "logistic loss 2.9446104701846925\n",
      "logistic loss 2.9122698771467093\n",
      "logistic loss 2.879173590938358\n",
      "logistic loss 2.8453353769825687\n",
      "logistic loss 2.8107716668361102\n",
      "logistic loss 2.7755016921610167\n",
      "logistic loss 2.7395475588644085\n",
      "logistic loss 2.7029342767835836\n",
      "logistic loss 2.665689753093236\n",
      "logistic loss 2.6278447538115635\n",
      "logistic loss 2.5894328357731276\n",
      "logistic loss 2.550490250416951\n",
      "logistic loss 2.511055820291487\n",
      "logistic loss 2.471170789082925\n",
      "logistic loss 2.430878646093284\n",
      "logistic loss 2.390224926356331\n",
      "logistic loss 2.3492569879253162\n",
      "logistic loss 2.308023768259513\n",
      "logistic loss 2.266575522045077\n",
      "logistic loss 2.2249635431861994\n",
      "logistic loss 2.183239874070164\n",
      "logistic loss 2.1414570055285633\n",
      "logistic loss 2.0996675711709574\n",
      "logistic loss 2.0579240399413683\n",
      "logistic loss 2.016278410837104\n",
      "logistic loss 1.9747819137244043\n",
      "logistic loss 1.9334718137211002\n",
      "logistic loss 2.1876999002656374\n",
      "logistic loss 2.2490736893519334\n",
      "logistic loss 2.295826755757259\n",
      "logistic loss 2.330577771558314\n",
      "logistic loss 2.355456723444627\n",
      "logistic loss 2.3721823379930655\n",
      "logistic loss 2.382141677143224\n",
      "logistic loss 2.386457959786494\n",
      "logistic loss 2.386044668778542\n",
      "logistic loss 2.3816476879760677\n",
      "logistic loss 2.3738778872924446\n",
      "logistic loss 2.3632363477148663\n",
      "logistic loss 2.3501339923166014\n",
      "logistic loss 2.3349069789581054\n",
      "logistic loss 2.317828870216427\n",
      "logistic loss 2.2991203295416756\n",
      "logistic loss 2.27895688876613\n",
      "logistic loss 2.2574751775743853\n",
      "logistic loss 2.234777888199689\n",
      "logistic loss 2.2109376578712645\n",
      "logistic loss 2.185999978194445\n",
      "logistic loss 2.1599851760468907\n",
      "logistic loss 2.132889445625781\n",
      "logistic loss 2.1046848347870912\n",
      "logistic loss 2.0753179846264183\n",
      "logistic loss 2.0447072626432554\n",
      "logistic loss 2.0127376680829707\n",
      "logistic loss 1.979252427069713\n",
      "logistic loss 1.9440393314378013\n",
      "logistic loss 1.9068081439097397\n",
      "logistic loss 1.8671516329579518\n",
      "logistic loss 1.8244737772328325\n",
      "logistic loss 1.7778439658287692\n",
      "logistic loss 1.725654573772595\n",
      "logistic loss 1.6646014525891848\n",
      "logistic loss 1.5847598978219595\n",
      "logistic loss 0.9503266081436573\n",
      "logistic loss 0.9234708096603315\n",
      "logistic loss 0.9015030325134391\n",
      "logistic loss 0.8801221619385169\n",
      "logistic loss 0.8593262186026477\n",
      "logistic loss 0.8391106608696437\n",
      "logistic loss 0.8194686783769116\n",
      "logistic loss 0.8003915594606756\n",
      "logistic loss 0.7818690589001072\n",
      "logistic loss 0.7638897334695944\n",
      "logistic loss 0.7464412337173961\n",
      "logistic loss 0.7295105506496767\n",
      "logistic loss 0.7130842206536544\n",
      "logistic loss 0.6971484937564452\n",
      "logistic loss 0.6816894706425376\n",
      "logistic loss 0.6666932135150434\n",
      "logistic loss 0.6521458352851265\n",
      "logistic loss 0.6380335709132898\n",
      "logistic loss 0.6243428340963\n",
      "logistic loss 0.6110602619382113\n",
      "logistic loss 0.5981727497704034\n",
      "logistic loss 0.5856674778899932\n",
      "logistic loss 0.5735319316636733\n",
      "logistic loss 0.5617539161791646\n",
      "logistic loss 0.5503215664130637\n",
      "logistic loss 0.5392233537106068\n",
      "logistic loss 0.5284480892325805\n",
      "logistic loss 0.5179849249108266\n",
      "logistic loss 0.5078233523616215\n",
      "logistic loss 0.49795320012984506\n",
      "logistic loss 0.48836462957615384\n",
      "logistic loss 0.47904812966749866\n",
      "logistic loss 0.46999451088981575\n",
      "logistic loss 0.4611948984665718\n",
      "logistic loss 0.45264072503799474\n",
      "logistic loss 0.44432372293101274\n",
      "logistic loss 0.43623591612997165\n",
      "logistic loss 0.42836961204062624\n",
      "logistic loss 0.42071739312510514\n",
      "logistic loss 0.41327210847350493\n",
      "logistic loss 0.4060268653670256\n",
      "logistic loss 0.39897502087821846\n",
      "logistic loss 0.39211017354677963\n",
      "logistic loss 0.3854261551623584\n",
      "logistic loss 0.37891702267982724\n",
      "logistic loss 0.3725770502886374\n",
      "logistic loss 0.3664007216523444\n",
      "logistic loss 0.36038272233213003\n",
      "logistic loss 0.35451793240410945\n",
      "logistic loss 0.3488014192778949\n",
      "logistic loss 0.34322843072205395\n",
      "logistic loss 0.33779438809940165\n",
      "logistic loss 0.3324948798140627\n",
      "logistic loss 0.32732565497067967\n",
      "logistic loss 0.3222826172448694\n",
      "logistic loss 0.3173618189631175\n",
      "logistic loss 0.3125594553895279\n",
      "logistic loss 0.30787185921626065\n",
      "logistic loss 0.303295495253577\n",
      "logistic loss 0.2988269553155085\n",
      "logistic loss 0.29446295329620487\n",
      "logistic loss 0.29020032043244426\n",
      "logistic loss 0.28603600074678376\n",
      "logistic loss 0.2819670466663616\n",
      "logistic loss 0.2779906148120275\n",
      "logistic loss 0.27410396195215714\n",
      "logistic loss 0.270304441116027\n",
      "logistic loss 0.2665894978610952\n",
      "logistic loss 0.2629566666889788\n",
      "logistic loss 0.2594035676048534\n",
      "logistic loss 0.25592790281486744\n",
      "logistic loss 0.252527453556705\n",
      "logistic loss 0.24920007705798575\n",
      "logistic loss 0.2459437036180449\n",
      "logistic loss 0.24275633380782427\n",
      "logistic loss 0.23963603578353396\n",
      "logistic loss 0.23658094270962282\n",
      "logistic loss 0.23358925028651556\n",
      "logistic loss 0.23065921437890433\n",
      "logistic loss 0.22778914874077796\n",
      "logistic loss 0.22497742283285327\n",
      "logistic loss 0.2222224597290145\n",
      "logistic loss 0.21952273410762893\n",
      "logistic loss 0.21687677032467992\n",
      "logistic loss 0.21428314056503978\n",
      "logistic loss 0.21174046306866873\n",
      "logistic loss 0.20924740042861362\n",
      "logistic loss 0.20680265795786437\n",
      "logistic loss 0.2044049821219801\n",
      "logistic loss 0.20205315903500587\n",
      "logistic loss 0.19974601301565567\n",
      "logistic loss 0.1974824052015768\n",
      "logistic loss 0.1952612322189405\n",
      "logistic loss 0.19308142490521374\n",
      "logistic loss 0.1909419470827705\n",
      "logistic loss 0.18884179438119636\n",
      "logistic loss 0.18677999310626844\n",
      "logistic loss 0.18475559915365136\n",
      "logistic loss 0.18276769696532658\n",
      "logistic loss 0.1808153985270744\n",
      "logistic loss 0.1788978424051028\n",
      "logistic loss 0.17701419282043834\n",
      "logistic loss 0.17516363875926527\n",
      "logistic loss 0.1733453931177319\n",
      "logistic loss 0.17155869187991088\n",
      "logistic loss 0.16980279332740952\n",
      "logistic loss 0.16807697727931634\n",
      "logistic loss 0.16638054436131636\n",
      "logistic loss 0.16471281530256868\n",
      "logistic loss 0.16307313025945133\n",
      "logistic loss 0.16146084816480957\n",
      "logistic loss 0.15987534610185775\n",
      "logistic loss 0.1583160187015381\n",
      "logistic loss 0.15678227756257437\n",
      "logistic loss 0.155273550693062\n",
      "logistic loss 0.15378928197290234\n",
      "logistic loss 0.15232893063618821\n",
      "logistic loss 0.15089197077253497\n",
      "logistic loss 0.1494778908470193\n",
      "logistic loss 0.14808619323742414\n",
      "logistic loss 0.14671639378862053\n",
      "logistic loss 0.14536802138295518\n",
      "logistic loss 0.14404061752631012\n",
      "logistic loss 0.14273373594913297\n",
      "logistic loss 0.14144694222164342\n",
      "logistic loss 0.1401798133829711\n",
      "logistic loss 0.13893193758346167\n",
      "logistic loss 0.13770291373966242\n",
      "logistic loss 0.13649235120157124\n",
      "logistic loss 0.13529986943155525\n",
      "logistic loss 0.13412509769453207\n",
      "logistic loss 0.13296767475909763\n",
      "logistic loss 0.131827248608867\n",
      "logistic loss 0.13070347616401476\n",
      "logistic loss 0.1295960230122941\n",
      "logistic loss 0.1285045631493915\n",
      "logistic loss 0.12742877872816852\n",
      "logistic loss 0.12636835981639605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic loss 0.12532300416282632\n",
      "logistic loss 0.12429241697105721\n",
      "logistic loss 0.12327631068119745\n",
      "logistic loss 0.12227440475866368\n",
      "logistic loss 0.12128642549018506\n",
      "logistic loss 0.12031210578652102\n",
      "logistic loss 0.11935118499181196\n",
      "logistic loss 0.11840340869912139\n",
      "logistic loss 0.11746852857213998\n",
      "logistic loss 0.11654630217267893\n",
      "logistic loss 0.11563649279382918\n",
      "logistic loss 0.1147388692984905\n",
      "logistic loss 0.11385320596323505\n",
      "logistic loss 0.11297928232705312\n",
      "logistic loss 0.11211688304513857\n",
      "logistic loss 0.11126579774718229\n",
      "logistic loss 0.11042582090030827\n",
      "logistic loss 0.10959675167626727\n",
      "logistic loss 0.1087783938229235\n",
      "logistic loss 0.10797055553969623\n",
      "logistic loss 0.10717304935693465\n",
      "logistic loss 0.10638569201912242\n",
      "logistic loss 0.10560830437162035\n",
      "logistic loss 0.10484071125099333\n",
      "logistic loss 0.10408274137867195\n",
      "logistic loss 0.10333422725791194\n",
      "logistic loss 0.10259500507392562\n",
      "logistic loss 0.10186491459697747\n",
      "logistic loss 0.10114379908853127\n",
      "logistic loss 0.10043150521018995\n",
      "logistic loss 0.09972788293531068\n",
      "logistic loss 0.09903278546342616\n",
      "logistic loss 0.09834606913708491\n",
      "logistic loss 0.09766759336121457\n",
      "logistic loss 0.09699722052492746\n",
      "logistic loss 0.0963348159255689\n",
      "logistic loss 0.09568024769500512\n",
      "logistic loss 0.09503338672814972\n",
      "logistic loss 0.09439410661346277\n",
      "logistic loss 0.0937622835655883\n",
      "logistic loss 0.09313779635982841\n",
      "logistic loss 0.09252052626861676\n",
      "logistic loss 0.09191035699976578\n",
      "logistic loss 0.09130717463647504\n",
      "logistic loss 0.09071086757906058\n",
      "logistic loss 0.09012132648835687\n",
      "logistic loss 0.08953844423072688\n",
      "logistic loss 0.0889621158245762\n",
      "logistic loss 0.08839223838842589\n",
      "logistic loss 0.08782871109041633\n",
      "logistic loss 0.08727143509920282\n",
      "logistic loss 0.08672031353624524\n",
      "logistic loss 0.0861752514294167\n",
      "logistic loss 0.08563615566786491\n",
      "logistic loss 0.0851029349581227\n",
      "logistic loss 0.08457549978147474\n",
      "logistic loss 0.08405376235236127\n",
      "logistic loss 0.08353763657807238\n",
      "logistic loss 0.08302703801941133\n",
      "logistic loss 0.08252188385245383\n",
      "logistic loss 0.08202209283137561\n",
      "logistic loss 0.08152758525218974\n",
      "logistic loss 0.08103828291754489\n",
      "logistic loss 0.08055410910242786\n",
      "logistic loss 0.0800749885207034\n",
      "logistic loss 0.0796008472926367\n",
      "logistic loss 0.0791316129132611\n",
      "logistic loss 0.07866721422145297\n",
      "logistic loss 0.0782075813699953\n",
      "logistic loss 0.07775264579627238\n",
      "logistic loss 0.07730234019381699\n",
      "logistic loss 0.07685659848450992\n",
      "logistic loss 0.07641535579157506\n",
      "logistic loss 0.07597854841321687\n",
      "logistic loss 0.0755461137969655\n",
      "logistic loss 0.07511799051460274\n",
      "logistic loss 0.0746941182378413\n",
      "logistic loss 0.07427443771450862\n",
      "logistic loss 0.07385889074537108\n",
      "logistic loss 0.07344742016159214\n",
      "logistic loss 0.07303996980265327\n",
      "logistic loss 0.07263648449489497\n",
      "logistic loss 0.0722369100305767\n",
      "logistic loss 0.07184119314746319\n",
      "logistic loss 0.07144928150886529\n",
      "logistic loss 0.07106112368423667\n",
      "logistic loss 0.0706766691302162\n",
      "logistic loss 0.07029586817211037\n",
      "logistic loss 0.06991867198589517\n",
      "logistic loss 0.06954503258053282\n",
      "logistic loss 0.06917490278087805\n",
      "logistic loss 0.06880823621084799\n",
      "logistic loss 0.0684449872770697\n",
      "logistic loss 0.06808511115291699\n",
      "logistic loss 0.06772856376290111\n",
      "logistic loss 0.0673753017674701\n",
      "logistic loss 0.06702528254810439\n",
      "logistic loss 0.06667846419283918\n",
      "logistic loss 0.06633480548208454\n",
      "logistic loss 0.06599426587477678\n",
      "logistic loss 0.06565680549484725\n",
      "logistic loss 0.06532238511807022\n",
      "logistic loss 0.06499096615910535\n",
      "logistic loss 0.06466251065894017\n",
      "logistic loss 0.06433698127256655\n",
      "logistic loss 0.06401434125694917\n",
      "logistic loss 0.06369455445929512\n",
      "logistic loss 0.06337758530555422\n",
      "logistic loss 0.0630633987891989\n",
      "logistic loss 0.06275196046027462\n",
      "logistic loss 0.06244323641466411\n",
      "logistic loss 0.062137193283599214\n",
      "logistic loss 0.0618337982234527\n",
      "logistic loss 0.06153301890570289\n",
      "logistic loss 0.06123482350712032\n",
      "logistic loss 0.06093918070025895\n",
      "logistic loss 0.06064605964403486\n",
      "logistic loss 0.060355429974601214\n",
      "logistic loss 0.060067261796388625\n",
      "logistic loss 0.05978152567335423\n",
      "logistic loss 0.059498192620410284\n",
      "logistic loss 0.05921723409505264\n",
      "logistic loss 0.05893862198917274\n",
      "logistic loss 0.058662328621023725\n",
      "logistic loss 0.05838832672738247\n",
      "logistic loss 0.05811658945587634\n",
      "logistic loss 0.0578470903574792\n",
      "logistic loss 0.05757980337915431\n",
      "logistic loss 0.057314702856644514\n",
      "logistic loss 0.057051763507471036\n",
      "logistic loss 0.05679096042400581\n",
      "logistic loss 0.05653226906674142\n",
      "logistic loss 0.056275665257698025\n",
      "logistic loss 0.05602112517394084\n",
      "logistic loss 0.0557686253412544\n",
      "logistic loss 0.05551814262796568\n",
      "logistic loss 0.055269654238841094\n",
      "logistic loss 0.05502313770918088\n",
      "logistic loss 0.05477857089897525\n",
      "logistic loss 0.054535931987221564\n",
      "logistic loss 0.05429519946632174\n",
      "logistic loss 0.05405635213664834\n",
      "logistic loss 0.05381936910114489\n",
      "logistic loss 0.05358422976013287\n",
      "logistic loss 0.053350913806126575\n",
      "logistic loss 0.053119401218811625\n",
      "logistic loss 0.05288967226012318\n",
      "logistic loss 0.052661707469390376\n",
      "logistic loss 0.05243548765862968\n",
      "logistic loss 0.05221099390786632\n",
      "logistic loss 0.05198820756061803\n",
      "logistic loss 0.05176711021939601\n",
      "logistic loss 0.05154768374139067\n",
      "logistic loss 0.05132991023412347\n",
      "logistic loss 0.0511137720512983\n",
      "logistic loss 0.05089925178865238\n",
      "logistic loss 0.05068633227994766\n",
      "logistic loss 0.05047499659298751\n",
      "logistic loss 0.05026522802574362\n",
      "logistic loss 0.05005701010256698\n",
      "logistic loss 0.04985032657043229\n",
      "logistic loss 0.04964516139530124\n",
      "logistic loss 0.049441498758526645\n",
      "logistic loss 0.049239323053326754\n",
      "logistic loss 0.049038618881339896\n",
      "logistic loss 0.04883937104923865\n",
      "logistic loss 0.04864156456540455\n",
      "logistic loss 0.04844518463668183\n",
      "logistic loss 0.048250216665158246\n",
      "logistic loss 0.04805664624506412\n",
      "logistic loss 0.047864459159648994\n",
      "logistic loss 0.047673641378216454\n",
      "logistic loss 0.047484179053113175\n",
      "logistic loss 0.04729605851686332\n",
      "logistic loss 0.047109266279279194\n",
      "logistic loss 0.046923789024680994\n",
      "logistic loss 0.046739613609159944\n",
      "logistic loss 0.0465567270578475\n",
      "logistic loss 0.046375116562313426\n",
      "logistic loss 0.046194769477926875\n",
      "logistic loss 0.046015673321324876\n",
      "logistic loss 0.04583781576790053\n",
      "logistic loss 0.045661184649361745\n",
      "logistic loss 0.04548576795129132\n",
      "logistic loss 0.04531155381080032\n",
      "logistic loss 0.04513853051418175\n",
      "logistic loss 0.04496668649464378\n",
      "logistic loss 0.04479601033005657\n",
      "logistic loss 0.04462649074074237\n",
      "logistic loss 0.044458116587320684\n",
      "logistic loss 0.04429087686858811\n",
      "logistic loss 0.04412476071940902\n",
      "logistic loss 0.04395975740868625\n",
      "logistic loss 0.043795856337340934\n",
      "logistic loss 0.043633047036326575\n",
      "logistic loss 0.04347131916468826\n",
      "logistic loss 0.04331066250766344\n",
      "logistic loss 0.043151066974791756\n",
      "logistic loss 0.04299252259807409\n",
      "logistic loss 0.042835019530174004\n",
      "logistic loss 0.04267854804261744\n",
      "logistic loss 0.04252309852406507\n",
      "logistic loss 0.04236866147856815\n",
      "logistic loss 0.042215227523906766\n",
      "logistic loss 0.04206278738992161\n",
      "logistic loss 0.04191133191686205\n",
      "logistic loss 0.041760852053807945\n",
      "logistic loss 0.04161133885708815\n",
      "logistic loss 0.04146278348871481\n",
      "logistic loss 0.041315177214890995\n",
      "logistic loss 0.04116851140448104\n",
      "logistic loss 0.041022777527558625\n",
      "logistic loss 0.040877967153957404\n",
      "logistic loss 0.04073407195183353\n",
      "logistic loss 0.040591083686297935\n",
      "logistic loss 0.04044899421800126\n",
      "logistic loss 0.04030779550181153\n",
      "logistic loss 0.04016747958548281\n",
      "logistic loss 0.04002803860831746\n",
      "logistic loss 0.03988946479991725\n",
      "logistic loss 0.03975175047890044\n",
      "logistic loss 0.03961488805166333\n",
      "logistic loss 0.03947887001114038\n",
      "logistic loss 0.0393436889356246\n",
      "logistic loss 0.03920933748756912\n",
      "logistic loss 0.03907580841242453\n",
      "logistic loss 0.0389430945374999\n",
      "logistic loss 0.03881118877082561\n",
      "logistic loss 0.03868008410005935\n",
      "logistic loss 0.03854977359136546\n",
      "logistic loss 0.038420250388390184\n",
      "logistic loss 0.03829150771115238\n",
      "logistic loss 0.0381635388550504\n",
      "logistic loss 0.03803633718980627\n",
      "logistic loss 0.03790989615847202\n",
      "logistic loss 0.037784209276459466\n",
      "logistic loss 0.037659270130517106\n",
      "logistic loss 0.03753507237782793\n",
      "logistic loss 0.037411609745031\n",
      "logistic loss 0.03728887602729232\n",
      "logistic loss 0.037166865087407054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic loss 0.03704557085489987\n",
      "logistic loss 0.036924987325110625\n",
      "logistic loss 0.036805108558363026\n",
      "logistic loss 0.03668592867908504\n",
      "logistic loss 0.03656744187494839\n",
      "logistic loss 0.03644964239608025\n",
      "logistic loss 0.036332524554195625\n",
      "logistic loss 0.036216082721826415\n",
      "logistic loss 0.03610031133151652\n",
      "logistic loss 0.035985204875031394\n",
      "logistic loss 0.03587075790259717\n",
      "logistic loss 0.03575696502214794\n",
      "logistic loss 0.03564382089857018\n",
      "logistic loss 0.03553132025296832\n",
      "logistic loss 0.03541945786195378\n",
      "logistic loss 0.035308228556910154\n",
      "logistic loss 0.03519762722330131\n",
      "logistic loss 0.03508764879999757\n",
      "logistic loss 0.034978288278573806\n",
      "logistic loss 0.03486954070263486\n",
      "logistic loss 0.03476140116717348\n",
      "logistic loss 0.03465386481790926\n",
      "logistic loss 0.03454692685065042\n",
      "logistic loss 0.03444058251065078\n",
      "logistic loss 0.03433482709201708\n",
      "logistic loss 0.03422965593705074\n",
      "logistic loss 0.034125064435696614\n",
      "logistic loss 0.034021048024899\n",
      "logistic loss 0.03391760218805792\n",
      "logistic loss 0.03381472245441462\n",
      "logistic loss 0.03371240439851394\n",
      "logistic loss 0.033610643639613774\n",
      "logistic loss 0.03350943584115851\n",
      "logistic loss 0.03340877671022383\n",
      "logistic loss 0.033308661996979204\n",
      "logistic loss 0.0332090874941575\n",
      "logistic loss 0.03311004903653912\n",
      "logistic loss 0.03301154250042236\n",
      "logistic loss 0.032913563803161246\n",
      "logistic loss 0.03281610890260733\n",
      "logistic loss 0.032719173796661345\n",
      "logistic loss 0.03262275452275658\n",
      "logistic loss 0.03252684715742048\n",
      "logistic loss 0.032431447815763126\n",
      "logistic loss 0.03233655265104723\n",
      "logistic loss 0.03224215785419384\n",
      "logistic loss 0.032148259653364375\n",
      "logistic loss 0.032054854313496514\n",
      "logistic loss 0.03196193813588823\n",
      "logistic loss 0.03186950745771981\n",
      "logistic loss 0.031777558651674837\n",
      "logistic loss 0.0316860881254983\n",
      "logistic loss 0.031595092321589885\n",
      "logistic loss 0.03150456771657728\n",
      "logistic loss 0.031414510820937\n",
      "logistic loss 0.03132491817858097\n",
      "logistic loss 0.03123578636647087\n",
      "logistic loss 0.031147111994229818\n",
      "logistic loss 0.031058891703765655\n",
      "logistic loss 0.030971122168884247\n",
      "logistic loss 0.030883800094927787\n",
      "logistic loss 0.030796922218402994\n",
      "logistic loss 0.03071048530663637\n",
      "logistic loss 0.030624486157393657\n",
      "logistic loss 0.030538921598547132\n",
      "logistic loss 0.030453788487716345\n",
      "logistic loss 0.03036908371194559\n",
      "logistic loss 0.030284804187347455\n",
      "logistic loss 0.030200946858775262\n",
      "logistic loss 0.030117508699509573\n",
      "logistic loss 0.030034486710911343\n",
      "logistic loss 0.029951877922127963\n",
      "logistic loss 0.02986967938974845\n",
      "logistic loss 0.029787888197528242\n",
      "logistic loss 0.02970650145605256\n",
      "logistic loss 0.0296255163024507\n",
      "logistic loss 0.02954492990009054\n",
      "logistic loss 0.0294647394382923\n",
      "logistic loss 0.029384942132023528\n",
      "logistic loss 0.029305535221624497\n",
      "logistic loss 0.029226515972529746\n",
      "logistic loss 0.02914788167497217\n",
      "logistic loss 0.029069629643726363\n",
      "logistic loss 0.02899175721780922\n",
      "logistic loss 0.02891426176025108\n",
      "logistic loss 0.028837140657798594\n",
      "logistic loss 0.02876039132066088\n",
      "logistic loss 0.028684011182261188\n",
      "logistic loss 0.02860799769897471\n",
      "logistic loss 0.028532348349875503\n",
      "logistic loss 0.028457060636506853\n",
      "logistic loss 0.028382132082588593\n",
      "logistic loss 0.02830756023383916\n",
      "logistic loss 0.02823334265769639\n",
      "logistic loss 0.028159476943080193\n",
      "logistic loss 0.02808596070018955\n",
      "logistic loss 0.028012791560238975\n",
      "logistic loss 0.027939967175258023\n",
      "logistic loss 0.02786748521784316\n",
      "logistic loss 0.027795343380976233\n",
      "logistic loss 0.0277235393777485\n",
      "logistic loss 0.027652070941201624\n",
      "logistic loss 0.027580935824079406\n",
      "logistic loss 0.02751013179863905\n",
      "logistic loss 0.027439656656425743\n",
      "logistic loss 0.027369508208074163\n",
      "logistic loss 0.027299684283120768\n",
      "logistic loss 0.027230182729779583\n",
      "logistic loss 0.027161001414765587\n",
      "logistic loss 0.02709213822308764\n",
      "logistic loss 0.027023591057861493\n",
      "logistic loss 0.026955357840121003\n",
      "logistic loss 0.026887436508626766\n",
      "logistic loss 0.026819825019684287\n",
      "logistic loss 0.026752521346955375\n",
      "logistic loss 0.026685523481291617\n",
      "logistic loss 0.026618829430533977\n",
      "logistic loss 0.02655243721935401\n",
      "logistic loss 0.02648634488907583\n",
      "logistic loss 0.02642055049750101\n",
      "logistic loss 0.026355052118739297\n",
      "logistic loss 0.026289847843049642\n",
      "logistic loss 0.026224935776649166\n",
      "logistic loss 0.02616031404159297\n",
      "logistic loss 0.026095980775555663\n",
      "logistic loss 0.02603193413173066\n",
      "logistic loss 0.025968172278625173\n",
      "logistic loss 0.025904693399924978\n",
      "logistic loss 0.025841495694353137\n",
      "logistic loss 0.025778577375488537\n",
      "logistic loss 0.025715936671634392\n",
      "logistic loss 0.025653571825661574\n",
      "logistic loss 0.025591481094869903\n",
      "logistic loss 0.025529662750836363\n",
      "logistic loss 0.025468115079274795\n",
      "logistic loss 0.02540683637989085\n",
      "logistic loss 0.02534582496624814\n",
      "logistic loss 0.025285079165621116\n",
      "logistic loss 0.025224597318868724\n",
      "logistic loss 0.02516437778029057\n",
      "logistic loss 0.025104418917496103\n",
      "logistic loss 0.02504471911127433\n",
      "logistic loss 0.02498527675546216\n",
      "logistic loss 0.02492609025681524\n",
      "logistic loss 0.024867158034886525\n",
      "logistic loss 0.02480847852188513\n",
      "logistic loss 0.02475005016256719\n",
      "logistic loss 0.024691871414107714\n",
      "logistic loss 0.02463394074597706\n",
      "logistic loss 0.02457625663982483\n",
      "logistic loss 0.024518817589353634\n",
      "logistic loss 0.02446162210021456\n",
      "logistic loss 0.02440466868987712\n",
      "logistic loss 0.024347955887518674\n",
      "logistic loss 0.024291482233918013\n",
      "logistic loss 0.02423524628134136\n",
      "logistic loss 0.024179246593419256\n",
      "logistic loss 0.024123481745056574\n",
      "logistic loss 0.024067950322301412\n",
      "logistic loss 0.024012650922263935\n",
      "logistic loss 0.023957582152984706\n",
      "logistic loss 0.023902742633352833\n",
      "logistic loss 0.023848130992986997\n",
      "logistic loss 0.023793745872141475\n",
      "logistic loss 0.023739585921597132\n",
      "logistic loss 0.023685649802566905\n",
      "logistic loss 0.023631936186595377\n",
      "logistic loss 0.023578443755466905\n",
      "logistic loss 0.02352517120110217\n",
      "logistic loss 0.023472117225459648\n",
      "logistic loss 0.02341928054044627\n",
      "logistic loss 0.023366659867829775\n",
      "logistic loss 0.023314253939124865\n",
      "logistic loss 0.02326206149553797\n",
      "logistic loss 0.023210081287834843\n",
      "logistic loss 0.02315831207628328\n",
      "logistic loss 0.02310675263055247\n",
      "logistic loss 0.023055401729621082\n",
      "logistic loss 0.02300425816169758\n",
      "logistic loss 0.022953320724138422\n",
      "logistic loss 0.02290258822334598\n",
      "logistic loss 0.022852059474712458\n",
      "logistic loss 0.022801733302511815\n",
      "logistic loss 0.022751608539827863\n",
      "logistic loss 0.022701684028472228\n",
      "logistic loss 0.022651958618906273\n",
      "logistic loss 0.022602431170166807\n",
      "logistic loss 0.022553100549775183\n",
      "logistic loss 0.022503965633664937\n",
      "logistic loss 0.022455025306107084\n",
      "logistic loss 0.022406278459640522\n",
      "logistic loss 0.022357723994979775\n",
      "logistic loss 0.022309360820961955\n",
      "logistic loss 0.02226118785444636\n",
      "logistic loss 0.02221320402027869\n",
      "logistic loss 0.02216540825117806\n",
      "logistic loss 0.02211779948769963\n",
      "logistic loss 0.02207037667814605\n",
      "logistic loss 0.022023138778503558\n",
      "logistic loss 0.02197608475237268\n",
      "logistic loss 0.0219292135708919\n",
      "logistic loss 0.02188252421269578\n",
      "logistic loss 0.0218360156638054\n",
      "logistic loss 0.02178968691761517\n",
      "logistic loss 0.021743536974773305\n",
      "logistic loss 0.021697564843163087\n",
      "logistic loss 0.02165176953781075\n",
      "logistic loss 0.021606150080832463\n",
      "logistic loss 0.021560705501366774\n",
      "logistic loss 0.021515434835522883\n",
      "logistic loss 0.02147033712631175\n",
      "logistic loss 0.021425411423576977\n",
      "logistic loss 0.021380656783953664\n",
      "logistic loss 0.021336072270798815\n",
      "logistic loss 0.02129165695412763\n",
      "logistic loss 0.02124740991056402\n",
      "logistic loss 0.02120333022328121\n",
      "logistic loss 0.021159416981938208\n",
      "logistic loss 0.02111566928263943\n",
      "logistic loss 0.021072086227852997\n",
      "logistic loss 0.02102866692638199\n",
      "logistic loss 0.02098541049329259\n",
      "logistic loss 0.020942316049868043\n",
      "logistic loss 0.020899382723553732\n",
      "logistic loss 0.02085660964789539\n",
      "logistic loss 0.02081399596250834\n",
      "logistic loss 0.020771540812995343\n",
      "logistic loss 0.020729243350924468\n",
      "logistic loss 0.020687102733758657\n",
      "logistic loss 0.020645118124810603\n",
      "logistic loss 0.020603288693199474\n",
      "logistic loss 0.02056161361379574\n",
      "logistic loss 0.02052009206716247\n",
      "logistic loss 0.020478723239527864\n",
      "logistic loss 0.02043750632272372\n",
      "logistic loss 0.020396440514138827\n",
      "logistic loss 0.020355525016677097\n",
      "logistic loss 0.020314759038707678\n",
      "logistic loss 0.020274141794014794\n",
      "logistic loss 0.020233672501760745\n",
      "logistic loss 0.02019335038644026\n",
      "logistic loss 0.02015317467782758\n",
      "logistic loss 0.020113144610931496\n",
      "logistic loss 0.0200732594259708\n",
      "logistic loss 0.020033518368305244\n",
      "logistic loss 0.019993920688408735\n",
      "logistic loss 0.019954465641825906\n",
      "logistic loss 0.01991515248912668\n",
      "logistic loss 0.019875980495856444\n",
      "logistic loss 0.01983694893251382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic loss 0.01979805707449485\n",
      "logistic loss 0.019759304202049747\n",
      "logistic loss 0.019720689600261288\n",
      "logistic loss 0.01968221255899197\n",
      "logistic loss 0.01964387237284473\n",
      "logistic loss 0.01960566834112088\n",
      "logistic loss 0.019567599767790232\n",
      "logistic loss 0.01952966596145271\n",
      "logistic loss 0.019491866235296303\n",
      "logistic loss 0.019454199907057698\n",
      "logistic loss 0.019416666298987046\n",
      "logistic loss 0.019379264737819406\n",
      "logistic loss 0.019341994554725836\n",
      "logistic loss 0.019304855085278703\n",
      "logistic loss 0.019267845669429746\n",
      "logistic loss 0.019230965651462283\n",
      "logistic loss 0.019194214379958614\n",
      "logistic loss 0.01915759120776222\n",
      "logistic loss 0.019121095491950423\n",
      "logistic loss 0.01908472659380064\n",
      "logistic loss 0.019048483878748294\n",
      "logistic loss 0.019012366716358885\n",
      "logistic loss 0.018976374480302538\n",
      "logistic loss 0.018940506548299624\n",
      "logistic loss 0.018904762302116296\n",
      "logistic loss 0.018869141127507563\n",
      "logistic loss 0.018833642414201202\n",
      "logistic loss 0.01879826555586439\n",
      "logistic loss 0.018763009950061103\n",
      "logistic loss 0.018727874998233755\n",
      "logistic loss 0.018692860105672147\n",
      "logistic loss 0.018657964681473987\n",
      "logistic loss 0.01862318813852521\n",
      "logistic loss 0.018588529893454164\n",
      "logistic loss 0.018553989366629813\n",
      "logistic loss 0.01851956598210499\n",
      "logistic loss 0.01848525916759826\n",
      "logistic loss 0.018451068354470593\n",
      "logistic loss 0.018416992977689554\n",
      "logistic loss 0.01838303247580314\n",
      "logistic loss 0.01834918629090824\n",
      "logistic loss 0.018315453868635843\n",
      "logistic loss 0.018281834658106295\n",
      "logistic loss 0.018248328111914\n",
      "logistic loss 0.018214933686100693\n",
      "logistic loss 0.01818165084011806\n",
      "logistic loss 0.018148479036812916\n",
      "logistic loss 0.018115417742399918\n",
      "logistic loss 0.018082466426422326\n",
      "logistic loss 0.018049624561750018\n",
      "logistic loss 0.0180168916245315\n",
      "logistic loss 0.017984267094173813\n",
      "logistic loss 0.01795175045333765\n",
      "logistic loss 0.017919341187876525\n",
      "logistic loss 0.0178870387868501\n",
      "logistic loss 0.017854842742472996\n",
      "logistic loss 0.017822752550096608\n",
      "logistic loss 0.017790767708198568\n",
      "logistic loss 0.017758887718338428\n",
      "logistic loss 0.01772711208514992\n",
      "logistic loss 0.01769544031631354\n",
      "logistic loss 0.017663871922529533\n",
      "logistic loss 0.017632406417503233\n",
      "logistic loss 0.01760104331791011\n",
      "logistic loss 0.017569782143383433\n",
      "logistic loss 0.01753862241649444\n",
      "logistic loss 0.017507563662711376\n",
      "logistic loss 0.017476605410413345\n",
      "logistic loss 0.017445747190821932\n",
      "logistic loss 0.017414988538025197\n",
      "logistic loss 0.017384328988924704\n",
      "logistic loss 0.017353768083226296\n",
      "logistic loss 0.017323305363421193\n",
      "logistic loss 0.017292940374761888\n",
      "logistic loss 0.017262672665240654\n",
      "logistic loss 0.01723250178557767\n",
      "logistic loss 0.01720242728918677\n",
      "logistic loss 0.01717244873216326\n",
      "logistic loss 0.017142565673271826\n",
      "logistic loss 0.017112777673903554\n",
      "logistic loss 0.01708308429809507\n",
      "logistic loss 0.01705348511246005\n",
      "logistic loss 0.01702397968621311\n",
      "logistic loss 0.016994567591131052\n",
      "logistic loss 0.016965248401532577\n",
      "logistic loss 0.016936021694270095\n",
      "logistic loss 0.016906887048693905\n",
      "logistic loss 0.01687784404665914\n",
      "logistic loss 0.016848892272488505\n",
      "logistic loss 0.01682003131295379\n",
      "logistic loss 0.016791260757270794\n",
      "logistic loss 0.016762580197070498\n",
      "logistic loss 0.016733989226387937\n",
      "logistic loss 0.016705487441645045\n",
      "logistic loss 0.01667707444162393\n",
      "logistic loss 0.016648749827462173\n",
      "logistic loss 0.016620513202626343\n",
      "logistic loss 0.016592364172905195\n",
      "logistic loss 0.016564302346383616\n",
      "logistic loss 0.016536327333428892\n",
      "logistic loss 0.01650843874668066\n",
      "logistic loss 0.016480636201018178\n",
      "logistic loss 0.016452919313565538\n",
      "logistic loss 0.01642528770366684\n",
      "logistic loss 0.016397740992859258\n",
      "logistic loss 0.016370278804872265\n",
      "logistic loss 0.016342900765610506\n",
      "logistic loss 0.016315606503130298\n",
      "logistic loss 0.01628839564762932\n",
      "logistic loss 0.016261267831431288\n",
      "logistic loss 0.016234222688974755\n",
      "logistic loss 0.016207259856784683\n",
      "logistic loss 0.01618037897348159\n",
      "logistic loss 0.016153579679737438\n",
      "logistic loss 0.016126861618285887\n",
      "logistic loss 0.016100224433892703\n",
      "logistic loss 0.016073667773351483\n",
      "logistic loss 0.016047191285461854\n",
      "logistic loss 0.0160207946210187\n",
      "logistic loss 0.01599447743280211\n",
      "logistic loss 0.015968239375549254\n",
      "logistic loss 0.01594208010596115\n",
      "logistic loss 0.015915999282674738\n",
      "logistic loss 0.015889996566253395\n",
      "logistic loss 0.015864071619176252\n",
      "logistic loss 0.015838224105817138\n",
      "logistic loss 0.015812453692440217\n",
      "logistic loss 0.015786760047182817\n",
      "logistic loss 0.015761142840041955\n",
      "logistic loss 0.0157356017428655\n",
      "logistic loss 0.015710136429329412\n",
      "logistic loss 0.015684746574941805\n",
      "logistic loss 0.015659431857014922\n",
      "logistic loss 0.015634191954658254\n",
      "logistic loss 0.015609026548770081\n",
      "logistic loss 0.01558393532201647\n",
      "logistic loss 0.015558917958830097\n",
      "logistic loss 0.015533974145391394\n",
      "logistic loss 0.015509103569615584\n",
      "logistic loss 0.015484305921143037\n",
      "logistic loss 0.01545958089132789\n",
      "logistic loss 0.015434928173226418\n",
      "logistic loss 0.015410347461590946\n",
      "logistic loss 0.015385838452839013\n",
      "logistic loss 0.015361400845072776\n",
      "logistic loss 0.015337034338032738\n",
      "logistic loss 0.015312738633119046\n",
      "logistic loss 0.01528851343335955\n",
      "logistic loss 0.015264358443405308\n",
      "logistic loss 0.015240273369520848\n",
      "logistic loss 0.015216257919568071\n",
      "logistic loss 0.015192311803004092\n",
      "logistic loss 0.015168434730861227\n",
      "logistic loss 0.015144626415747408\n",
      "logistic loss 0.015120886571823252\n",
      "logistic loss 0.015097214914801232\n",
      "logistic loss 0.015073611161931564\n",
      "logistic loss 0.015050075031987433\n",
      "logistic loss 0.015026606245268026\n",
      "logistic loss 0.0150032045235756\n",
      "logistic loss 0.014979869590208073\n",
      "logistic loss 0.014956601169952575\n",
      "logistic loss 0.014933398989078769\n",
      "logistic loss 0.014910262775320763\n",
      "logistic loss 0.01488719225786403\n",
      "logistic loss 0.014864187167356437\n",
      "logistic loss 0.014841247235871463\n",
      "logistic loss 0.014818372196924199\n",
      "logistic loss 0.014795561785443712\n",
      "logistic loss 0.014772815737771028\n",
      "logistic loss 0.014750133791648957\n",
      "logistic loss 0.014727515686215368\n",
      "logistic loss 0.014704961161988658\n",
      "logistic loss 0.014682469960860266\n",
      "logistic loss 0.014660041826092523\n",
      "logistic loss 0.01463767650230234\n",
      "logistic loss 0.014615373735452469\n",
      "logistic loss 0.014593133272845057\n",
      "logistic loss 0.014570954863117637\n",
      "logistic loss 0.014548838256221334\n",
      "logistic loss 0.01452678320342704\n",
      "logistic loss 0.014504789457308985\n",
      "logistic loss 0.014482856771736026\n",
      "logistic loss 0.014460984901867324\n",
      "logistic loss 0.014439173604138615\n",
      "logistic loss 0.01441742263626272\n",
      "logistic loss 0.014395731757204978\n",
      "logistic loss 0.014374100727201583\n",
      "logistic loss 0.014352529307720557\n",
      "logistic loss 0.014331017261479699\n",
      "logistic loss 0.014309564352423894\n",
      "logistic loss 0.014288170345720212\n",
      "logistic loss 0.014266835007753867\n",
      "logistic loss 0.014245558106117278\n",
      "logistic loss 0.014224339409603083\n",
      "logistic loss 0.014203178688193421\n",
      "logistic loss 0.014182075713059444\n",
      "logistic loss 0.014161030256549545\n",
      "logistic loss 0.014140042092180567\n",
      "logistic loss 0.014119110994629917\n",
      "logistic loss 0.014098236739738226\n",
      "logistic loss 0.014077419104481965\n",
      "logistic loss 0.014056657866986847\n",
      "logistic loss 0.014035952806513576\n",
      "logistic loss 0.014015303703442806\n",
      "logistic loss 0.013994710339278174\n",
      "logistic loss 0.013974172496634837\n",
      "logistic loss 0.013953689959234735\n",
      "logistic loss 0.013933262511894945\n",
      "logistic loss 0.013912889940530448\n",
      "logistic loss 0.013892572032130356\n",
      "logistic loss 0.013872308574774155\n",
      "logistic loss 0.013852099357605454\n",
      "logistic loss 0.013831944170830649\n",
      "logistic loss 0.013811842805718622\n",
      "logistic loss 0.013791795054591724\n",
      "logistic loss 0.013771800710809105\n",
      "logistic loss 0.013751859568775062\n",
      "logistic loss 0.013731971423924592\n",
      "logistic loss 0.013712136072716954\n",
      "logistic loss 0.013692353312628448\n",
      "logistic loss 0.013672622942156985\n",
      "logistic loss 0.013652944760799528\n",
      "logistic loss 0.013633318569055624\n",
      "logistic loss 0.013613744168415147\n",
      "logistic loss 0.013594221361368708\n",
      "logistic loss 0.013574749951373626\n",
      "logistic loss 0.013555329742874885\n",
      "logistic loss 0.013535960541285015\n",
      "logistic loss 0.013516642152977295\n",
      "logistic loss 0.013497374385286795\n",
      "logistic loss 0.013478157046499458\n",
      "logistic loss 0.013458989945851962\n",
      "logistic loss 0.013439872893516946\n",
      "logistic loss 0.013420805700605483\n",
      "logistic loss 0.013401788179157744\n",
      "logistic loss 0.013382820142140355\n",
      "logistic loss 0.013363901403430405\n",
      "logistic loss 0.013345031777829098\n",
      "logistic loss 0.013326211081035388\n",
      "logistic loss 0.013307439129656557\n",
      "logistic loss 0.013288715741194116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic loss 0.013270040734040287\n",
      "logistic loss 0.013251413927473624\n",
      "logistic loss 0.013232835141651985\n",
      "logistic loss 0.013214304197611184\n",
      "logistic loss 0.013195820917256757\n",
      "logistic loss 0.01317738512335345\n",
      "logistic loss 0.01315899663953082\n",
      "logistic loss 0.013140655290272168\n",
      "logistic loss 0.013122360900912861\n",
      "logistic loss 0.01310411329762341\n",
      "logistic loss 0.013085912307423632\n",
      "logistic loss 0.013067757758160492\n",
      "logistic loss 0.01304964947851534\n",
      "logistic loss 0.013031587297990024\n",
      "logistic loss 0.013013571046909814\n",
      "logistic loss 0.012995600556411737\n",
      "logistic loss 0.01297767565843894\n",
      "logistic loss 0.012959796185752494\n",
      "logistic loss 0.01294196197189738\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000000):\n",
    "    print_loss = False\n",
    "    if i % 1000 == 0:\n",
    "        print_loss = True\n",
    "    \n",
    "    model.update_weights([0, 1, 0, 1, 1, 1, 1, 1],[0, 0, 1, 0, 1, 1, 1, 1], learning_rate=0.01, print_loss=print_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31773c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(x_input):\n",
    "    output = []\n",
    "    \n",
    "    for x in x_input:\n",
    "        if x < 0.50:\n",
    "            output.append(0)\n",
    "        else:\n",
    "            output.append(1)\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f53e3d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarize(model.predict([0, 1, 0, 1, 1, 1, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "283829bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarize(model.predict([1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8c99822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.373542276608194e-16,\n",
       " 0.0032888399065128344,\n",
       " 1.0,\n",
       " 0.0032756576472684393,\n",
       " 0.9977901439457997,\n",
       " 0.9986052741477939,\n",
       " 0.9986300230577,\n",
       " 0.9986309764993976]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([0, 1, 0, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d923e17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarize( model.predict([0, 1, 0, 1, 1, 1, 1, 1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e4956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
